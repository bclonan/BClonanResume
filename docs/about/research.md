---
title: Research
---

## Overview

## Table of Contents

[[toc]]

## Neuropsychology

> the study of the relationship between behavior, emotion, and cognition on the one hand, and brain function on the other.

### Light Before Sound

#### **Topic**

Traversing through the vast expansion of neural networks in the human brain is an adventure pioneering scientists are on every day. With each new explanation a world of possibilities are open. One specific area of study that has helped us understand the way humans thrive, and survive is how we communicate. When communicating humans use many parts of the brain; for example try to say the word “cat” allowed. Completing such a simple task as speaking a word allowed is a testament to the complexity of the brain; first information must travel to the primary visual cortex, than the information is sent to the posterior speech area, after it takes a ride along the arcuate fasciculus to Broca’s area, finally making its way to the primary motor cortex, and out of your mouth all in a matter of milliseconds! Mapping the way we process sound and communicate have aided the healthcare field in finding new treatments for patients suffering for many different types of neurological disorders.

#### **Background**

Localization of function is a blanket term used to describe specific functions are served by specific areas of the brain (Fischl & Dale, 2006). Currently there are several areas of the brain known to aid humans in producing and comprehending visual or audio cues as language. Located in the frontal lobe in the left hemisphere is Broca’s area which serves as the motor area for producing language, lesions to this area produces a form of aphasia aptly named Broca’s Aphasia which can prevent a person from producing properly formed speech, though the person can still understand language. (Paul Broca, 1861). Wernicke’s area is located in the temporal lobe is involved in the comprehension of language, lesions in this area produces the loss of the ability to understand language; unlike Broca’s aphasia a sufferer can speak clearly but the words produced make no sense, and are jumbled (Carl Wernicke, 1879). Broca’s area and Wernicke’s area are connected together by the arcuate fasciculus a bundle of nerve fibers. Brain imaging techniques have identified another area that is connected to both the Broca’s area, and Wernicke’s area the parietal lobule. The parietal lobule lies at the junction of the auditory, visual, and somatosensory cortexes. In addition to being in this unique area the neurons in this lobule are multimodal (able to process different kinds of stimuli). The primary auditory cortex is used to receive spoken language, and the primary visual cortex is used to receive a word that is read. Viewing brain activation patterns of studies interested in determining the difference between how we use imagery “imagining a stimulus” and perception “viewing stimuli” of shown or spoken stimuli correlate to activation patterns in Broca’s and Wernicke’s areas. Furthermore, as with comprehending spoken, and written speech it was found that activity in the visual cortex in the occipital lobe resulted in the best prediction for what their subjects were perceiving. Activity in higher visual areas was the best predictor of what their subjects were imagining which in language processing the production area (Sue-Hynn Lee ET. All, 2012). When receiving either visual or audio stimuli of language we form mental images of these stimuli. It is easier to perceive audio presentations of words when in context than when taken out of context (Waldrop, 1988). Similar to audio it is easier to find letters when they are presented in a word than if they are scattered or in a non-word (Gerald Reicher 1969). Visual stimuli play a role in effecting our interpretation of an audio sentence (Michael tanenhaus and coworkers, 1995).

#### **Hypothesis**

Currently the hierarchical manor of importance in processing language overlooks the visual aspect in many areas.

Processing language, sound, and visual stimuli together is apparent in all forms of communication. However, in auditory studies the visual aspect is often overlooked. Evidence from aphasic patients point towards visual (seen or perceived) being the primary “meaning maker” for processing both audio and visual language. Considering damage to the Broca’s area (motor control) prevents a person speaking language; yet allows them to still understand what is being said. Damage to Wernicke’s area (organizer) prevents a person from understanding language, and also prevents them from forming coherent sentences. A person who is blind can be taught to read with brail, a death person can be taught to communicate with sign language. All forms of communication require visual interpretations. Human’s process language and meaning in a hierarchical manor for both audio, and non-audio scenarios. Alternative hypothesis there is no difference in visual processing speed paired with audio.

**Brief Desctiption of experiment's**

- Experiment - Syllable match up

In this experiment I will be using a decision task which will consist of three different sessions. First being an audio presentation of a random word, followed by a screen to choose how many syllables were heard in the word. In the second visual words of varying lengths will flash on the screen, followed by a screen to choose how many syllables were in the flashed word. Condition three will consist of audio and visual stimuli presented at the same time, followed by a screen to choose how many syllables were in the word. The final condition will be an unpaired or paired matchup of a spoken and written word produced at the same time, first the participant will choose the spoken word, and second another screen will come up and choose the written word. Response times and accuracy will be measured in these tasks .The prediction is that the response time on visual processing will be faster and more accurate than the audio presentation. Also that when presented together it will take longer to respond due to visual, and audio being utilized. Furthermore the unmatched and matched pair test will help understand the accuracy of using both audio, and visual together.

- Methods

#### **Participants**

One Rutgers university male student age 26 with no known visual, or audio hearing problems participated in this experiment to fulfill a course requirement.

#### **Stimulus**

There are three types of stimuli used. For condition one the audio stimuli consists of a 1- 2second soundbite of a word chosen at random from the following list (dog, cat, rat, tree, into, nose, without, grandmother, potato, water, telephone, February, desk, jump, light, pencil, funny, triangle, underwater, independent, march, grade, starfish, eleven, sailboat, candle, watermelon) the audio versions of the word were created with a computer generated male voice. The visual stimuli consisted of a randomly chosen word from the same list that was used for the audio presentation. The visual image was font size 20 black times new roman font presented in the center of the screen on a white background for 2 seconds. The third stimuli used in the second screen for all of the decision tasks consisted of a list in descending order of numbers 1-4 in black time’s new roman font size 20.

#### **Design**

A repeated measure design with the same participant completing all of the conditions will be used. The words that will be used in all of the tasks will be chosen from the same ( dog, cat, rat, tree, into, nose, without, grandmother, potato, water, telephone, February, desk, jump, light, pencil, funny, triangle, underwater, independent, march, grade, starfish, eleven, sailboat, candle, watermelon, pumpkin, lamp, hat, basketball, red, nature, chew, stick, attack, yawn, sleeping, swimming, windfall ) and will be randomized . The independent measure in the first condition was the number of syllables in the words being spoken varying from 1-4 syllables per word. In the first condition the dependent variable was measuring accuracy, response time spent on the second screen shown while picking the number of syllables defined as time (in seconds). The independent measure in the second condition was the number of syllables displayed in the flashed word. The dependent variable was measuring accuracy, and response time spent on the second screen. The third independent variable will be number of syllables in both the audio/visual flashed word. The dependent variable will be accuracy, and response time spent on the second screen. The fourth independent variable is presenting an unpaired or paired form of the visual and auditory word at the same time. The dependent variable in the fourth condition is measuring accuracy and time of both answers.

#### **Design**

#### **Design**

#### **Design**

#### **Table 2 : Reaction times compared**

| Reaction Time Compared    | Show statistical significance (yes, no) | Null hypothesis there is no difference between response times. |
| :------------------------ | :-------------------------------------: | -------------------------------------------------------------: |
| Audio solo vs visual solo |                  None                   |                                          t(26)=0.07, .94 > .05 |
| Visual solo vs paired     |                  None                   |                                        t(26)=-0.82, 0.41 > .05 |
| Audio solo vs paired      |                  None                   |                                         t(26)=-8.23, 1.02> .05 |
|                           |                                         |                                                                |
|                           |                                         |                                                                |
|                           |                                         |                                                                |

#### **Interpretation based on key findings.**

Currently the hierarchical manor of importance in processing language overlooks the visual aspect in many areas.

The visual aspect of language plays a major role in all aspects of comprehending and giving meaning to language. My hypothesis that language is processed in a hierarchical manor with vision playing a major roll is supported. By studying the reaction times compared to the other conditions it is apparent that in all cases language is processed by imagining at some point with a visual aspect. When comparing the audio only trial to the mixed audio visual trial a blocking of the visual aspect occurs slowing down reaction time and lowering accuracy. When comparing the visual only reaction time to the mixed condition the audio component blocks the ability to comprehend the visually seen word which leads to lower accuracy, and reaction time. Participants were able to realize how many syllables much more efficiently and at nearly the same reaction time when the paired set of stimuli were presents compared to the baselines. It is likely that producing and comprehending language is a constant guess check process rather than previously believed set path to follow.

#### **Comparisons/contrasts to other studies**

Localization of function does still seem apparent in the processing and comprehending of language, however due to the ability to still respond fairly quickly with over 80% accuracy it is likely that in the background our previously believed major language areas can work as dual purpose production or comprehension areas; or much more likely as a constant circuit team reflecting information through bundled nerve fibers constantly. With that in mind it is likely that Broca’s area, Wernicke’s area, and the arcuate fasciculus are all connected to our visual directly, and separately. It is said that a person with Brocas aphasia cannot produce sound however they can still understand it, this would support the idea that the visual component is the meaning maker. Patients suffering from Wernicke’s aphasia are limited in being able to produce clearly or understand language; often speaking in “word salad” jumbled mixes of syllables therefore Wernicke’s area must serve as the visual “meaning makers” spell checker and syllable organizer by the time the information gets to the Broca’s area like in my mixed word study participants accuracy and comprehension suffers because one area of the visual loop is in this case missing. The multimodal neurons located in the parietal lobe are likely located along our language/visual loop which allow us to do things such as tasting our favorite food when one utters the word, or visually picturing an image of how letters are arranged when spelling a word. Prior studies on perceiving visual stimuli have told us that the best predictor for what subjects were perceiving when presented with an image or asked to imagine a word help support the idea that the occipital lobe is a primary force in all language comprehension due to its help in giving meaning to words. When receiving either visual or audio stimuli of language we form mental images of these stimuli. It is easier to perceive audio presentations of words when in context than when taken out of context; by including the importance of the visual component in all language meaning production, and choosing to block that meaning maker out accuracy and response time was slowed down similar to taking them out of context. Similar to audio it is easier to find letters when they are presented in a word than if they are scattered or in a non-word. The constant hierarchical guess and check language system proposed would support the difficulty in finding those non words. Visual stimuli play a role on our ability to process a sentence because there must be a constant relay of information between the audio/visual loops.

#### **Limitations of study**

The sample size of participants is a limitation in the study, with a larger sample size one could infer better as a whole how the audio/visual language pathway functions. A voice recognition task to measure response times on auditory production could assist in showing that all the parts are constantly bouncing information between each other. A future direction would be to enlarge the sample size, and complete the trials while utilizing a brain imaging machine such as an fMRI or EEG. Future experiments would benefit in finding more ways to block these processes or slow reaction times too help understand the overall functions of each area.

#### **New hypotheses and ideas**

Based off of the results of reaction times, and accuracy testing a definite hierarchical language process happens, if you remove one of the components or occupy one of the pieces the reaction time and accuracy is slowed down. Contradictory to the previous belief of the visual component not being used in auditory process of hearing a language the evidence of the mixed vs paired tasks shows otherwise; this is exemplified by the ability to process a word easily and at near same rate/accuracy when it is the same written as it is audio if this were not true than mixed processing should be producing similar numbers to the other conditions. With an fMRI machine or a mind numbing techniques a better understanding to the extent of which our audio/visual language processing loops can compensate for each other. Furthermore, because of the multimodal neurons located in these areas new brain training techniques to aid in therapy of patients suffering from diseases like aphasia.

#### **Conclusion**

Overall the blocking of one of the audio/visual language loop components can greatly reduce accuracy and response time. The visual component of language is the meaning maker, Wernicke’s are is the sorter, and Broca’s area is the motor area the language area can likely become overwhelmed with too many stimuli inhibiting our ability to process stimuli such as language. By studying the multimodal neurons located in these components new language learning techniques can be implemented. By studying the practical uses of other senses to compensate for language areas options like teaching meaning of words through sign language or picture presentation therapy may be viable sources of help.
